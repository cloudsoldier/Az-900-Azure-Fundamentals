
# Lab: Creating a Pipeline to Transfer Data from Azure Data Lake to SQL Data Warehouse

## Objective
Learn how to use **Azure Synapse Analytics** to create a pipeline that transfers data from an **Azure Data Lake Gen2 Storage Account** to a **SQL Data Warehouse** (SQL pool). This lab demonstrates how to process and move data using the **Copy Data tool** in Synapse Studio.

---

## Key Concepts

1. **Azure Synapse Studio**:
   - A unified interface for managing data, pipelines, and SQL pools.
   - Allows for data movement, analytics, and monitoring in one platform.

2. **SQL Data Warehouse (SQL Pool)**:
   - A scalable SQL-based solution within Azure Synapse for analytical workloads.

3. **Data Pipeline**:
   - A set of processes to move data from a source (e.g., Data Lake) to a destination (e.g., SQL pool).

4. **Copy Data Tool**:
   - A wizard-based tool to simplify data transfer between a source and a destination.

---

## Step-by-Step Lab

### Step 1: Verify SQL Pool Deployment
1. Navigate to your **Azure Synapse Workspace** in the Azure Portal.
2. Confirm that your **SQL pool** has been deployed.
   - This SQL pool will act as your **SQL Data Warehouse**.

---

### Step 2: Open Synapse Studio
1. In the Synapse Workspace, click on **Open Synapse Studio**.
2. Explore the features:
   - **Data**: View and manage storage accounts and SQL pools.
   - **Integrate**: Create and manage pipelines.
   - **Monitor**: Monitor pipeline execution.

---

### Step 3: Create a Pipeline with the Copy Data Tool
1. **Navigate to the Integrate Section**:
   - In Synapse Studio, go to **Integrate**.
   - Click **+** and select **Copy Data tool**.

2. **Configure Source**:
   - Select **Run task now** and click **Next**.
   - Choose the source of your data:
     - Click **New Connection** and select **Azure Data Lake Storage Gen2**.
     - Browse to locate the **log.csv** file in your Data Lake.
     - Confirm the file as your source.

3. **Review Source Data**:
   - The tool detects the file format (e.g., CSV).
   - Optionally, preview the data to confirm its structure.
   - Click **Next**.

---

### Step 4: Configure Destination
1. **Select Destination**:
   - Choose the **SQL pool** in your Synapse Workspace as the destination.
   - If no table exists in the SQL pool:
     - Allow the wizard to create a new table automatically.

2. **Column Mapping**:
   - Review the column mapping between the source and destination.
   - Leave the default mapping as it is.
   - Click **Next**.

---

### Step 5: Complete and Run the Pipeline
1. **Configure Task Settings**:
   - Provide a name for the task (e.g., `LogDataTransfer`).
   - Choose the data transfer method as **Bulk Insert** for simplicity.

2. **Run the Pipeline**:
   - Click **Next** to deploy the pipeline.
   - The tool creates the required compute infrastructure and executes the pipeline.

3. **Monitor Execution**:
   - Go to the **Monitor** section in Synapse Studio.
   - Verify that the pipeline status shows **Succeeded**.

---

### Step 6: Verify Data Transfer
1. **Explore Data in SQL Pool**:
   - In Synapse Studio, go to the **Data** section.
   - Expand your SQL pool and view the newly created table (e.g., `log`).

2. **Query the Table**:
   - Right-click the table and select **New SQL Script > SELECT TOP 100 rows**.
   - Run the query to view the transferred data.

---

### Step 7: Clean Up Resources
1. **Delete Resources**:
   - Go to the Azure Portal and navigate to **All Resources**.
   - Delete the **Synapse Workspace** and **SQL pool** to avoid charges.
   - Retain the **Azure Data Lake Storage Account** for future use.

---

## Notes
- **Pipeline Features**:
  - The **Copy Data tool** simplifies data movement but does not replace advanced pipelines for complex workflows.
- **SQL Data Warehouse Usage**:
  - The SQL pool is optimized for analytics and can handle large datasets efficiently.
- **Cost Management**:
  - Ensure unused resources are deleted to minimize costs.

---

## Summary
In this lab, you created a pipeline using the **Copy Data tool** in **Synapse Studio** to transfer data from an **Azure Data Lake** to a **SQL Data Warehouse**. This workflow demonstrates how to use Synapse Analytics for data integration and analysis in a simple, streamlined manner.
